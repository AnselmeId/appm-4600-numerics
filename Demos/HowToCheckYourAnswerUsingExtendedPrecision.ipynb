{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HowToCheckYourAnswerUsingExtendedPrecision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOY7ESaK5DWDVPz9G8ByRsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbeckr/numerical-analysis-class/blob/master/Demos/HowToCheckYourAnswerUsingExtendedPrecision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR9zGjKZsK01",
        "colab_type": "text"
      },
      "source": [
        "# Checking your answer using extended precision\n",
        "\n",
        "This is an example of using higher precision to get a \"true answer\" and then using that to check roundoff error for lower precision.\n",
        "\n",
        "Let's look at $$f(x)=e^x - 1$$ which is tricky when $$x\\approx 0$$ due to subtractive cancellation.\n",
        "\n",
        "We can evaluate this with the naive method, as well as `numpy.expm1`. Let's see which is more accurate.\n",
        "\n",
        "To be extra careful that the $1$ is in the appropriate precision, we'll write a function like\n",
        "$$\n",
        "F(x,y) = e^x - y\n",
        "$$\n",
        "and then use\n",
        "$$\n",
        "f(x) = F(x,1).\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "$$\n",
        "\n",
        "Note, in the code below, the `float128` type doesn't necessarily have 128 bits, it might have 80 bits and be padded (see [Numpy data types](https://numpy.org/doc/stable/user/basics.types.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuKoY8IfktN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7c6510e3-e7a4-49fc-8c7a-96c60095e70c"
      },
      "source": [
        "import numpy as np\n",
        "precisionTypes = [np.longdouble,np.float64,np.float32,np.float16]\n",
        "\n",
        "# Try different sizes for x.  We'll start in the highest precision,\n",
        "#   and later reduce precision.\n",
        "x = np.array( 1e-4, dtype=np.longdouble )\n",
        "\n",
        "f = lambda x, y : np.exp(x) - y  # y = 1\n",
        "g = lambda x, y : np.expm1(x)    # the -1 part is included in expm1\n",
        "fx, gx = [], []\n",
        "for prec in precisionTypes:\n",
        "  fx.append(f(np.array(x,dtype=prec),np.array(1,dtype=prec)) )\n",
        "  gx.append(g(np.array(x,dtype=prec),np.array(1,dtype=prec)) )\n",
        "fx, gx\n",
        "\n",
        "trueAnswer = gx[0]\n",
        "relAccuracy = lambda x : np.abs(x-trueAnswer)/np.abs(trueAnswer)\n",
        "numDigits   = lambda x : -np.log10( relAccuracy(x) + 1e-18 )\n",
        "for i,prec in enumerate(precisionTypes):\n",
        "  print(\"For float{:<3d}, rel. err of f is {:5.2e} and of g is {:5.2e}\".format(int(128/(2**i)),relAccuracy(fx[i]),relAccuracy(gx[i])))\n",
        "print(\"\\nPut another way, here's the number of correct digits:\\n\")  \n",
        "for i,prec in enumerate(precisionTypes):\n",
        "  print(\"For float{:<3d}, f has {:4.1f} correct digits, g has {:4.1f} correct digits\".format(int(128/(2**i)),numDigits(fx[i]),numDigits(gx[i])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For float128, rel. err of f is 3.44e-18 and of g is 0.00e+00\n",
            "For float64 , rel. err of f is 4.33e-13 and of g is 3.44e-18\n",
            "For float32 , rel. err of f is 1.16e-04 and of g is 4.11e-08\n",
            "For float16 , rel. err of f is 1.00e+00 and of g is 1.16e-04\n",
            "\n",
            "Put another way, here's the number of correct digits:\n",
            "\n",
            "For float128, f has 17.4 correct digits, g has 18.0 correct digits\n",
            "For float64 , f has 12.4 correct digits, g has 17.4 correct digits\n",
            "For float32 , f has  3.9 correct digits, g has  7.4 correct digits\n",
            "For float16 , f has -0.0 correct digits, g has  3.9 correct digits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stND_ggstMcb",
        "colab_type": "text"
      },
      "source": [
        "Conclusion: the relative error in the \"g\" version of the expression is consistently a lot smaller. The second expression really is more stable"
      ]
    }
  ]
}
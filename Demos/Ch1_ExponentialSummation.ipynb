{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch1_SymbolicTaylorSeries.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsKZ/ST9it2e3/nxHnlMWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/appm-4600-numerics/blob/main/Demos/Ch1_ExponentialSummation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNDLwuliDDql"
      },
      "source": [
        "In [Ch1_SymbolicTaylorSeries](Ch1_SymbolicTaylorSeries.ipynb) we saw how to use the Taylor remainder theorem to estimate error in an expansion, and this is useful if you want to calculate a function like sine or cosine using only additions/subtractions and multiplications/division.\n",
        "\n",
        "Let's try that with the exponential function.  This is a classic example in numerical analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoGrsMYdCXq6"
      },
      "source": [
        "# Load the package\n",
        "import numpy as np\n",
        "import sympy as sym\n",
        "from sympy import init_printing\n",
        "init_printing()  # This will make output look very nice!\n",
        "\n",
        "import math  # has things like math.pi (which is the numerical value of pi)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ3odjv1Dt6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aeb8137a-9801-4af9-bc22-b49f911eaed4"
      },
      "source": [
        "# First, all symbolic variables must be declared. This is in contrast to Mathematica\n",
        "x = sym.Symbol('x')\n",
        "\n",
        "# Recall the Taylor series for exp(x)... very simple!\n",
        "sym.series( sym.exp(-x), x )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         2    3    4    5         \n",
              "        x    x    x    x      ⎛ 6⎞\n",
              "1 - x + ── - ── + ── - ─── + O⎝x ⎠\n",
              "        2    6    24   120        "
            ],
            "text/latex": "$\\displaystyle 1 - x + \\frac{x^{2}}{2} - \\frac{x^{3}}{6} + \\frac{x^{4}}{24} - \\frac{x^{5}}{120} + O\\left(x^{6}\\right)$"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuQIA1W-so8V"
      },
      "source": [
        "Let's try to find $e^{-30}$ with about 14 digits of precision.  Recally that Taylor's remainder theorem says that the error in the Taylor series (with $n-1$ terms) is the $n$th term but evaluate the derivative at some $\\xi \\in [x_0,x]$.  We'll expand around $0$, so $x_0=0$ (we do this because we *know* that $e^0=1$; we could choose a different $x_0$ to expand about if we knew what $e^{x_0}$ is).\n",
        "\n",
        "For this remainder term, the $n$th derivative of $e^{-x}$ is $(-1)^ne^{-x}$, and since our target $x=30$ is positive, we know $\\xi \\ge 0$ and hence $e^{-\\xi} \\le 1$, so we can bound this derivative term by $1$ (in absolute value).  We also need to include the coefficient $1/n!$ and the polynomial term $(x-x_0)^n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-2mgqeZsyR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c84c4b-8f15-4cf5-e2d5-6ca42f84fd85"
      },
      "source": [
        "X   = -30\n",
        "\n",
        "# Taylor's remainder theorem says that the remainder term at the (n-1) step is the nth Taylor series term but evaluated at some \\xi\n",
        "Rem = lambda n : abs( X**n / math.factorial(n) )\n",
        "for n in range(45):\n",
        "  print(f'Remainder(n={n:2d}) is {Rem(n):.2e}')\n",
        "  # When n is large enough, eventually this sequence will start to decrease"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remainder(n= 0) is 1.00e+00\n",
            "Remainder(n= 1) is 3.00e+01\n",
            "Remainder(n= 2) is 4.50e+02\n",
            "Remainder(n= 3) is 4.50e+03\n",
            "Remainder(n= 4) is 3.38e+04\n",
            "Remainder(n= 5) is 2.02e+05\n",
            "Remainder(n= 6) is 1.01e+06\n",
            "Remainder(n= 7) is 4.34e+06\n",
            "Remainder(n= 8) is 1.63e+07\n",
            "Remainder(n= 9) is 5.42e+07\n",
            "Remainder(n=10) is 1.63e+08\n",
            "Remainder(n=11) is 4.44e+08\n",
            "Remainder(n=12) is 1.11e+09\n",
            "Remainder(n=13) is 2.56e+09\n",
            "Remainder(n=14) is 5.49e+09\n",
            "Remainder(n=15) is 1.10e+10\n",
            "Remainder(n=16) is 2.06e+10\n",
            "Remainder(n=17) is 3.63e+10\n",
            "Remainder(n=18) is 6.05e+10\n",
            "Remainder(n=19) is 9.55e+10\n",
            "Remainder(n=20) is 1.43e+11\n",
            "Remainder(n=21) is 2.05e+11\n",
            "Remainder(n=22) is 2.79e+11\n",
            "Remainder(n=23) is 3.64e+11\n",
            "Remainder(n=24) is 4.55e+11\n",
            "Remainder(n=25) is 5.46e+11\n",
            "Remainder(n=26) is 6.30e+11\n",
            "Remainder(n=27) is 7.00e+11\n",
            "Remainder(n=28) is 7.50e+11\n",
            "Remainder(n=29) is 7.76e+11\n",
            "Remainder(n=30) is 7.76e+11\n",
            "Remainder(n=31) is 7.51e+11\n",
            "Remainder(n=32) is 7.04e+11\n",
            "Remainder(n=33) is 6.40e+11\n",
            "Remainder(n=34) is 5.65e+11\n",
            "Remainder(n=35) is 4.84e+11\n",
            "Remainder(n=36) is 4.03e+11\n",
            "Remainder(n=37) is 3.27e+11\n",
            "Remainder(n=38) is 2.58e+11\n",
            "Remainder(n=39) is 1.99e+11\n",
            "Remainder(n=40) is 1.49e+11\n",
            "Remainder(n=41) is 1.09e+11\n",
            "Remainder(n=42) is 7.79e+10\n",
            "Remainder(n=43) is 5.43e+10\n",
            "Remainder(n=44) is 3.70e+10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZDdLHB2580s"
      },
      "source": [
        "So if we want 14 digits, let's calculate a bound on the number of Taylor series we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46WrYmBXtlHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a715b2d3-1100-44bc-a64b-c9a75b072163"
      },
      "source": [
        "tol = 1.0e-14\n",
        "n = 0\n",
        "while Rem(n) > tol and n < 1e8 : # this last check is to prevent infinite loops, which are a possibility due to bugs\n",
        "  n += 1\n",
        "N = n-1\n",
        "print( \"We need at least\", N, \"terms, since Rem(n+1) is %.2e\" % Rem(N+1) )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We need at least 106 terms, since Rem(n+1) is 9.19e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUv1tXQp6DI1"
      },
      "source": [
        "Now, let's sum the Taylor series. We should have an error of 1e-14, right?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9bs9oT5vSU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd67222-d698-4146-91ae-928a198182d3"
      },
      "source": [
        "e = 0.\n",
        "for n in range(N+1):\n",
        "  e += X**n / math.factorial(n)\n",
        "  if ( not (n % 10 ) ) or ( n > N-10 ):  # save some output...\n",
        "    print(\"Iteration %3d\"% n, \" estimate is %29.15f\"% e, \"and error is %9.3e\" % abs(e - math.exp(X)) )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration   0  estimate is             1.000000000000000 and error is 1.000e+00\n",
            "Iteration  10  estimate is     121254849.571428596973419 and error is 1.213e+08\n",
            "Iteration  20  estimate is   85291712210.032180786132812 and error is 8.529e+10\n",
            "Iteration  30  estimate is  384842612535.907775878906250 and error is 3.848e+11\n",
            "Iteration  40  estimate is   63336540117.266479492187500 and error is 6.334e+10\n",
            "Iteration  50  estimate is     878229229.277898311614990 and error is 8.782e+08\n",
            "Iteration  60  estimate is       1685584.300283375196159 and error is 1.686e+06\n",
            "Iteration  70  estimate is           622.524572911353516 and error is 6.225e+02\n",
            "Iteration  80  estimate is             0.055867710574729 and error is 5.587e-02\n",
            "Iteration  90  estimate is            -0.000084070677839 and error is 8.407e-05\n",
            "Iteration  97  estimate is            -0.000085530630269 and error is 8.553e-05\n",
            "Iteration  98  estimate is            -0.000085530022814 and error is 8.553e-05\n",
            "Iteration  99  estimate is            -0.000085530206891 and error is 8.553e-05\n",
            "Iteration 100  estimate is            -0.000085530151668 and error is 8.553e-05\n",
            "Iteration 101  estimate is            -0.000085530168071 and error is 8.553e-05\n",
            "Iteration 102  estimate is            -0.000085530163247 and error is 8.553e-05\n",
            "Iteration 103  estimate is            -0.000085530164652 and error is 8.553e-05\n",
            "Iteration 104  estimate is            -0.000085530164246 and error is 8.553e-05\n",
            "Iteration 105  estimate is            -0.000085530164362 and error is 8.553e-05\n",
            "Iteration 106  estimate is            -0.000085530164330 and error is 8.553e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9_TYrmy6IRI"
      },
      "source": [
        "... but our error is in fact much worse. What went wrong?  Our estimate of the error was due to our mathematical approximation of the exponential, but we didn't include numerical error due to the summation.  We are adding and subtracting a lot of numbers of similar sizes, so we loose precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLrv54ATy0IW"
      },
      "source": [
        "Attempted fix #1: use better summation strategies.  Unfortunately, we'll see that this doesn't quite fix it for us.\n",
        "- Fix 1: use Python's builtin `fsum`, which does a version of compensated summation (related to [Kahan summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm), which is a trick that effectively let's us work with twice as many digits)\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVEh4BM61LXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "753bf551-fb0d-4dd1-dc69-97d6b9f4d061"
      },
      "source": [
        "# Fix 1a.\n",
        "# See https://www.math.ubc.ca/~pwalls/math-python/python/sequences/\n",
        "series = [ X**n / math.factorial(n) for n in range(N+1) ]  # \"lazy\", or \"iterable\". This is a list comprehension\n",
        "e = math.fsum( series )\n",
        "abs( e - math.exp(X) )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.843294459828891e-05"
            ],
            "text/latex": "$\\displaystyle 6.84329445982889 \\cdot 10^{-5}$"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = math.fsum( reversed(series) ) # no better\n",
        "# e = sum( series ) # same as fsum (as of Python 3.12, sum() uses Neumaier summation)\n",
        "# e = np.array(series).sum() % worse\n",
        "abs( e - math.exp(X) )"
      ],
      "metadata": {
        "id": "y8rwupArtsv5",
        "outputId": "a23083e2-bf6b-4460-a88f-3c210eefa0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.843294459828891e-05"
            ],
            "text/latex": "$\\displaystyle 6.84329445982889 \\cdot 10^{-5}$"
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEAP7T1swwd_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f7841ab1-8717-4dd9-eb14-fd3e698a1e17"
      },
      "source": [
        "# Kahan Summation, though doesn't work (Python might be optimizing?)\n",
        "e = 0.\n",
        "accum = 0.\n",
        "for n in range(N+1):\n",
        "  a = X**n / math.factorial(n) - accum\n",
        "  b = e + a\n",
        "  accum = (b-e) -a\n",
        "  e = b\n",
        "  if n > N-4:\n",
        "    print(\"Iteration %2d\"% n, \" estimate is %21.15f\"% e, \"and error is %9.3e\" % abs(e - math.exp(X)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 103  estimate is    -0.000085530164652 and error is 8.553e-05\n",
            "Iteration 104  estimate is    -0.000085530164246 and error is 8.553e-05\n",
            "Iteration 105  estimate is    -0.000085530164362 and error is 8.553e-05\n",
            "Iteration 106  estimate is    -0.000085530164330 and error is 8.553e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyti5Jrf18Tt"
      },
      "source": [
        "Attempted fix #2: use some math.\n",
        "- Fix 2a: split into positive and negative terms, add those separately, then combine at the end. Good idea, but again, doesn't quite fix it.\n",
        "- Fix 2b: rewrite $e^{-x} = \\frac{1}{e^x} = \\frac{1}{1+x+x^2/2+\\ldots}$.  Finally, we have a good solution!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCJ1vHKPzo8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "b6a03192-8761-42b6-8107-6b2e27dcad41"
      },
      "source": [
        "# Fix 2a\n",
        "seriesEven = [ X**n / math.factorial(n) for n in range(0,N+1,2) ]\n",
        "seriesOdd = [ X**n / math.factorial(n) for n in range(1,N+1,2) ]\n",
        "e = math.fsum( seriesEven ) + math.fsum( seriesOdd )\n",
        "e = math.fsum( reversed(seriesEven) ) + math.fsum( reversed(seriesOdd) ) # doesn't help much\n",
        "abs( e - math.exp(X) )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0009765625000935762"
            ],
            "text/latex": "$\\displaystyle 0.000976562500093576$"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to do this, in NumPy:\n",
        "s = np.array(series)\n",
        "with np.printoptions(precision=20):\n",
        "  print( (s[::2].sum() + s[1::2].sum()) - math.exp(X))"
      ],
      "metadata": {
        "id": "ocqIMxh1uXi7",
        "outputId": "675fda33-54b0-4809-f097-f4c1514ddd79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009765624999064238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veG534Ry2eZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "4ff8a409-f014-4a19-fc20-477314310390"
      },
      "source": [
        "# Fix 2b\n",
        "series = [ abs(X)**n / math.factorial(n) for n in range(N+1) ]\n",
        "e = 1./math.fsum( series )\n",
        "abs( e - math.exp(X) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/latex": "$$1.262177448353619e-29$$",
            "text/plain": [
              "1.262177448353619e-29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}